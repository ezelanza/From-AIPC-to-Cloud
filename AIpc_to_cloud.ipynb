{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From AIPC to the Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll train a CIFAR-10 image classification model using AI-powered computing (AIPC) and cloud resources. CIFAR-10 is a benchmark dataset with 60,000 images across 10 classes. Leveraging cloud infrastructure enables faster training and scalability.\n",
    "\n",
    "We'll cover dataset preprocessing, CNN model design, training with AIPC, and evaluation. This guide provides insights into optimizing model performance and deploying in production.\n",
    "\n",
    "Extracted from Pytorch example https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considerations \n",
    "\n",
    "Training a model with a limited amount of data is a pragmatic approach, especially if you're in the early stages of model development or data collection. It allows you to quickly iterate and experiment with different model architectures, hyperparameters, and feature engineering techniques to identify what works best for your problem domain.\n",
    "\n",
    "Once you have a good understanding of what works and what doesn't, scaling up to a larger dataset can indeed be beneficial. More data often leads to better generalization and performance, as it helps the model learn more robust patterns and relationships within the data.\n",
    "\n",
    "However, there are a few considerations to keep in mind:\n",
    "\n",
    "- **Quality of Data**: Ensure that your limited dataset is representative of the broader population or distribution you're targeting. Biases or outliers in the limited dataset can skew the model's understanding and lead to poor generalization when scaling up.\n",
    "\n",
    "- **Overfitting**: With a limited dataset, there's a risk of overfittingâ€”where the model learns to memorize the training data rather than generalize to unseen examples. Regularization techniques and validation strategies should be employed to mitigate this risk.\n",
    "\n",
    "- **Transferability**: Ideally, the insights gained from training on the limited dataset should transfer well to the larger dataset. However, there may be domain shifts or differences in data distribution between the two datasets, requiring careful adaptation and fine-tuning of the model.\n",
    "- **Computational Resources**: Training with a larger dataset typically requires more computational resources in terms of processing power, memory, and time. Ensure you have the necessary infrastructure to handle the increased scale.\n",
    "\n",
    "In summary, starting with a limited dataset for initial experimentation is a great approach, but it's essential to validate the model's performance on a larger dataset to ensure robustness and generalization. Additionally, continuous monitoring and refinement may be necessary as you scale up to address any new challenges or nuances introduced by the larger dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install pylibjpeg-libjpeg\n",
    "!pip install libpng-bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and prepare data sets for AIPC and Cloud examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll work with two versions of the CIFAR-10 dataset:\n",
    "\n",
    "1. **Short Dataset (for AI-powered computing)**:\n",
    "   This version comprises a subset of the CIFAR-10 dataset, containing a smaller number of images per class. It's designed for quick experimentation and testing on AI-powered computing resources.\n",
    "\n",
    "2. **Complete Dataset (for cloud computing)**:\n",
    "   The complete CIFAR-10 dataset consists of 60,000 images, evenly distributed across 10 classes. This version provides comprehensive training data and is suitable for training on cloud computing resources.\n",
    "\n",
    "By using these two datasets, we can efficiently utilize the resources available on both AI-powered computing and cloud platforms, ensuring rapid experimentation and robust model training.\n",
    "\n",
    "Let's load the datasets and proceed with our analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For Cloud we will use the entire Dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset_Cloud = torchvision.datasets.CIFAR10(root='./data_cloud', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader_Cloud = torch.utils.data.DataLoader(trainset_Cloud, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset_cloud = torchvision.datasets.CIFAR10(root='./data_cloud', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader_cloud = torch.utils.data.DataLoader(testset_cloud, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Display each dataset size\n",
    "print(\"Cloud Trainset size: \", len(trainset_Cloud),\" images\")\n",
    "print(\"Cloud Testset size: \", len(testset_cloud),\" images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For AI PC we will use a portion (40%  20,000) of the initial dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Initialize the CIFAR-10 dataset and download it\n",
    "full_dataset = torchvision.datasets.CIFAR10(root='./data_ai_pc', train=True,\n",
    "                                             download=True, transform=transform)\n",
    "\n",
    "# Size of the first subset\n",
    "train_size_AI_PC = 20000\n",
    "\n",
    "# Get the labels for each sample in the full dataset\n",
    "labels = torch.tensor(full_dataset.targets)\n",
    "\n",
    "# Use stratified sampling to split the dataset into train and test sets\n",
    "train_indices, test_indices = train_test_split(range(len(full_dataset)), train_size=train_size_AI_PC,test_size=2000,\n",
    "                                               stratify=labels, random_state=42)\n",
    "\n",
    "# Create the train and test subsets using the selected indices\n",
    "trainset_AI_PC = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "testset_AI_PC = torch.utils.data.Subset(full_dataset, test_indices)\n",
    "\n",
    "# Create the dataloaders\n",
    "batch_size = 4\n",
    "trainloader_AI_PC = torch.utils.data.DataLoader(trainset_AI_PC, batch_size=batch_size, shuffle=True)\n",
    "testloader_AI_PC = torch.utils.data.DataLoader(testset_AI_PC, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print the number of samples in each subset\n",
    "print(\"Number of samples in trainset_AI_PC:\", len(trainset_AI_PC))\n",
    "print(\"Number of samples in testset_AI_PC:\", len(testset_AI_PC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader_AI_PC)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.  Define Neural Network and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we define a convolutional neural network (CNN) model using PyTorch. Below is a breakdown of the model definition:\n",
    "\n",
    "1. **Module Definition**: The model is defined as a subclass of `nn.Module`, the base class for all neural network modules in PyTorch.\n",
    "\n",
    "2. **Initialization Method (`__init__`)**:\n",
    "   - The `__init__` method initializes the neural network layers, including convolutional layers, max-pooling layers, and fully connected layers.\n",
    "   - It sets up the architecture of the model, comprising two convolutional layers (`conv1` and `conv2`), two max-pooling layers (`pool`), and three fully connected layers (`fc1`, `fc2`, and `fc3`).\n",
    "\n",
    "3. **Forward Method (`forward`)**:\n",
    "   - The `forward` method defines the forward pass of the network, specifying how input data flows through the layers to produce the output.\n",
    "   - It applies convolutional operations, ReLU activation functions, max-pooling, and flattening to transform the input data.\n",
    "   - Finally, it computes the output of the neural network.\n",
    "   \n",
    "This model architecture is a common design for image classification tasks, leveraging convolutional layers to extract features from input images and fully connected layers for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training and Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will train and test our CIFAR-10 model, named `model_ipex_XXXX` (_aipc/cloud) \n",
    "\n",
    "### Training Process\n",
    "1. **Data Loading**: We'll load the training dataset and set up data loaders to provide batches of images and labels to the model during training.\n",
    "2. **Model Initialization**: Initialize an instance of our neural network model `model_ipex_XXX`.\n",
    "3. **Optimizer Configuration**: We'll set up an optimizer for training. In this case, we'll use the `ipex.optimize` function provided by IPEX to optimize our model, utilizing the `torch.bfloat16` data type.\n",
    "4. **Training Loop**: Iterate over the training dataset, passing batches of data through the model, computing gradients, and updating model parameters to minimize the loss.\n",
    "5. **Monitoring Progress**: Monitor training metrics such as loss and accuracy to assess the model's performance during training.\n",
    "\n",
    "### Testing Process\n",
    "1. **Data Loading**: Load the testing dataset to evaluate the trained model's performance.\n",
    "2. **Model Evaluation**: Pass testing data through the trained model and compute predictions.\n",
    "3. **Performance Metrics**: Calculate performance metrics such as accuracy to evaluate the model's performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 AI PC \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Training \n",
    "The NN will be trained using intel IPEX for aditional optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import intel_extension_for_pytorch as ipex\n",
    "import torch.optim as optim\n",
    "\n",
    "model_ipex_aipc = Net()\n",
    "\n",
    "#Convert model to run on GPU\n",
    "model_ipex_aipc = model_ipex_aipc.to('xpu')\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_ipex_aipc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Invoke optimize function against the model object and optimizer object\n",
    "model_ipex_aipc, optimizer = ipex.optimize(model_ipex_aipc, optimizer=optimizer, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate execution time\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader_AI_PC, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        \n",
    "        inputs, labels = data\n",
    "        \n",
    "        #Convert Data Set to run on GPU\n",
    "        inputs = inputs.to('xpu',dtype=torch.bfloat16)\n",
    "        labels = labels.to('xpu')\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model_ipex_aipc(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 700 == 699:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "#calculate execution time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f'Execution time: {execution_time} seconds')\n",
    "print(f'Length of the dataset: {len(trainset_AI_PC)}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader_AI_PC:\n",
    "        images, labels = data\n",
    "        outputs = model_ipex_aipc(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.1  Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import intel_extension_for_pytorch as ipex\n",
    "import torch.optim as optim\n",
    "\n",
    "model_ipex_cloud = Net()\n",
    "\n",
    "#Convert model to run on GPU\n",
    "model_ipex_cloud = model_ipex_cloud.to('xpu')\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_ipex_cloud.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Invoke optimize function against the model object and optimizer object\n",
    "model_ipex_cloud, optimizer = ipex.optimize(model_ipex_cloud, optimizer=optimizer, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate execution time\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader_Cloud, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        \n",
    "        inputs, labels = data\n",
    "        \n",
    "        #Convert Data Set to run on GPU\n",
    "        inputs = inputs.to('xpu',dtype=torch.bfloat16)\n",
    "        labels = labels.to('xpu')\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model_ipex_aipc(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 700 == 699:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "#calculate execution time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f'Execution time: {execution_time} seconds')\n",
    "print(f'Length of the dataset: {len(trainset_AI_PC)}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader_cloud:\n",
    "        images, labels = data\n",
    "        outputs = model_ipex_cloud(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "# print accuracy for each class            \n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
